{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6h4t_O8_4MK9"
      },
      "outputs": [],
      "source": [
        "# Relevant imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Resnet from scratch.\n",
        "\n",
        "### For more info check out this paper: https://arxiv.org/abs/1512.03385 which introduces the architecture.\n",
        "\n",
        "### High level:\n",
        "\n",
        "Really deep architectures suffer in training performance, this means that the network is suffering for reasons other than overfitting. The reason is mostly due to exploding or vanishing gradients. Resnets handle this by including the input of a layer (the previous layer's output) as into the output of the previous layer such that if the gradient explodes or vanishes, the input is still added.\n",
        "\n",
        "Below I reproduced the model and ran it on the CIFAR data set.\n"
      ],
      "metadata": {
        "id": "LLmj8obo4XhC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgIU1hJS5KEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride=1, downsample=None):\n",
        "    ## a basic residual block is defined as...\n",
        "    ### convolution layer 1, batchnorm1 relu (or some other activation) convolution 2, batch norm, then downsample if needed.\n",
        "    ### Note you will only down sample if stride ir padding !=1\n",
        "    super(BasicBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels,out_channels, stride=stride, kernel_size = 3,padding=1,bias=False)\n",
        "    self.batchnorm1 = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.conv2 = nn.Conv2d(out_channels,out_channels, stride=1, kernel_size = 3,padding=1,bias=False)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
        "    self.downsample = downsample\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    identity = x\n",
        "    out= self.conv1(x)\n",
        "    out=self.batchnorm1(out)\n",
        "    out= self.relu(out)\n",
        "    out= self.conv2(out)\n",
        "    out = self.batchnorm2(out)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      identity = self.downsample(x)\n",
        "\n",
        "    out+=identity\n",
        "    out = self.relu(out)\n",
        "    return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ResNet18, self).__init__()\n",
        "\n",
        "    self.initial_conv = nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1)\n",
        "    self.bn1 = nn.BatchNorm2d(64)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.first_subgroup = self._make_layer(64, 64, stride=1)\n",
        "\n",
        "    self.second_subgroup = self._make_layer(64, 128, stride=2)\n",
        "\n",
        "\n",
        "    self.third_subgroup = self._make_layer(128, 256,stride=2)\n",
        "\n",
        "    self.fourth_subgroup = self._make_layer(256, 512, stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "    self.linear = nn.Linear(512,10)\n",
        "\n",
        "  def _make_layer(self, in_channels, out_channels, blocks=2, stride=1):\n",
        "      downsample = None\n",
        "      if stride != 1 or in_channels != out_channels:\n",
        "          downsample = nn.Sequential(\n",
        "              nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "          )\n",
        "\n",
        "      layers = []\n",
        "      layers.append(BasicBlock(in_channels, out_channels, stride, downsample))\n",
        "      for _ in range(1, blocks):\n",
        "          layers.append(BasicBlock(out_channels, out_channels))\n",
        "\n",
        "      return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.initial_conv(x)\n",
        "    x = self.bn1(x)\n",
        "    x= self.relu(x)\n",
        "    x= self.first_subgroup(x)\n",
        "    x= self.second_subgroup(x)\n",
        "    x= self.third_subgroup(x)\n",
        "    x= self.fourth_subgroup(x)\n",
        "    x  = self.avgpool(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    output = self.linear(x)\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "nKWwAmcF4SNz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def create_dataloader(dataset, shuffle=True, batch_size=128, num_workers=2):\n",
        "    return DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "\n",
        "## applying the transformations they did in the paper\n",
        "def get_dataset(datapath, batch_size=128, num_workers=2):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "    # Load the CIFAR-10 dataset with the transformations\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=datapath,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = create_dataloader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return train_loader\n",
        "\n"
      ],
      "metadata": {
        "id": "Rrbmle3-5DLY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "\n",
        "  device = torch.device(args.device)\n",
        "\n",
        "  model = ResNet18().to(device)\n",
        "\n",
        "  optimizer = args.optimizer\n",
        "\n",
        "  if optimizer == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "  elif optimizer == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args.lr,momentum=args.momentum, weight_decay=args.weight_decay)\n",
        "  print(optimizer)\n",
        "\n",
        "  # Load CIFAR10 data\n",
        "  workers = args.device\n",
        "  train_loader = get_dataset(args.datapath,args.batch_size,args.workers)\n",
        "\n",
        "  # Training Loop\n",
        "  epochs = args.epochs\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "\n",
        "        output = model(batch_x)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(output, batch_y)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the running loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Predictions and accuracy\n",
        "        _, predicted = torch.max(output, 1)  # Get the class with the highest score (Top-1)\n",
        "        correct_predictions += (predicted == batch_y).sum().item()  # Count correct predictions\n",
        "        total_predictions += batch_y.size(0)  # Total number of samples in this batch\n",
        "\n",
        "\n",
        "    # Compute the average loss and accuracy for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct_predictions / total_predictions\n",
        "\n",
        "    print({\"Per epoch top 1\":(accuracy)})\n",
        "    print({\"Per epoch average loss\":(avg_loss)})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Hz6YNvG50UA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wanted to try argparse (to simulate linux machine running model)"
      ],
      "metadata": {
        "id": "6apJirdy7Pd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"CIFAR-10 Training Example with DataLoader\")\n",
        "\n",
        "    parser.add_argument('--optimizer', type=str, choices=['adam', 'sgd'], default='adam', help='Optimizer type (adam or sgd)')\n",
        "    parser.add_argument('--device', type=str, choices=['cuda', 'cpu'], default='cuda', help='Device to train on')\n",
        "    parser.add_argument('--datapath', type=str, default='./data', help='Path to the dataset')\n",
        "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size for training')\n",
        "    parser.add_argument('--workers', type=int, default=2, help='Number of data loading workers')\n",
        "    parser.add_argument('--epochs', type=int, default=5, help='Number of data loading workers')\n",
        "    parser.add_argument('--lr', type=float, default=0.1, help='Learning rate (default: 0.1)')\n",
        "    parser.add_argument('--momentum', type=float, default=0.9, help='Momentum for SGD optimizer (default: 0.9)')\n",
        "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 penalty) (default: 5e-4)')\n",
        "\n",
        "\n",
        "    # Parse the arguments\n",
        "    args = parser.parse_known_args()\n",
        "    print(args[0])\n",
        "\n",
        "    my_dataloader = create_dataloader(args[0].datapath, args[0].batch_size, args[0].workers)\n",
        "\n",
        "    print(f\"Using device: {args[0].device}\")\n",
        "    print(f\"Using optimizer: {args[0].optimizer}\")\n",
        "\n",
        "    # Call Training\n",
        "    train(args[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EiNrrdiq7F4S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSHQ0us67GUu",
        "outputId": "83eff500-7695-48c8-c04b-3fe83a393e95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(optimizer='adam', device='cuda', datapath='./data', batch_size=128, workers=2, epochs=5, lr=0.1, momentum=0.9, weight_decay=0.0005)\n",
            "Using device: cuda\n",
            "Using optimizer: adam\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.1\n",
            "    maximize: False\n",
            "    weight_decay: 0.0005\n",
            ")\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 12879375.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "{'Per epoch top 1': 22.262}\n",
            "{'Per epoch average loss': 2.217677504815104}\n",
            "{'Per epoch top 1': 29.288}\n",
            "{'Per epoch average loss': 1.8525692733657328}\n",
            "{'Per epoch top 1': 29.882}\n",
            "{'Per epoch average loss': 1.8284326861886417}\n",
            "{'Per epoch top 1': 30.526}\n",
            "{'Per epoch average loss': 1.8183220164550236}\n",
            "{'Per epoch top 1': 31.334}\n",
            "{'Per epoch average loss': 1.8140286448056742}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_8j6a4z7KHH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}